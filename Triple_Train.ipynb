{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "german-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader  \n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "planned-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triplet_Dataset import TripletData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "developed-begin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "infectious-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset =  TripletData('training_triples.csv')\n",
    "trainloader= torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "blocked-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoding_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoding_Network, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(7, 32, kernel_size=2), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=2), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2,2))\n",
    "        self.linear = nn.Linear(576, 256)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.linear(x))\n",
    "      \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "catholic-power",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8417, 0.0000, 0.0000, 0.1411, 0.0000, 1.0802, 1.6015, 0.6298, 0.0000,\n",
       "         0.1423, 0.0000, 0.0000, 0.6015, 0.0000, 0.1212, 0.6173, 0.0000, 1.2153,\n",
       "         0.0000, 0.7775, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1852, 0.1623,\n",
       "         1.1759, 1.0392, 0.0000, 0.0000, 0.0531, 0.0000, 0.0000, 0.0811, 0.8081,\n",
       "         0.5548, 0.7960, 0.0000, 0.0000, 0.6627, 0.7294, 0.1574, 0.9837, 0.0000,\n",
       "         0.4418, 0.0000, 0.1674, 0.0521, 1.1730, 0.5207, 0.0000, 0.0000, 0.5048,\n",
       "         0.7715, 1.5921, 0.9661, 0.2274, 0.6439, 0.1013, 0.0000, 0.5392, 1.2827,\n",
       "         0.1592, 0.1052, 0.0965, 0.4440, 0.0000, 0.8552, 0.0671, 0.0000, 0.2993,\n",
       "         0.0000, 0.0000, 0.5037, 0.0000, 0.0000, 0.0000, 0.0000, 0.5714, 1.0979,\n",
       "         0.6194, 0.0000, 0.9292, 0.0000, 0.9772, 1.0031, 0.1566, 0.9796, 0.0000,\n",
       "         0.6631, 0.0000, 0.0000, 0.6284, 0.0000, 0.0000, 0.5403, 0.0000, 0.1231,\n",
       "         0.2631, 0.3462, 0.0278, 0.9065, 0.5997, 0.0000, 0.0000, 0.0000, 1.1555,\n",
       "         0.2930, 0.0000, 0.0000, 0.0000, 0.6601, 0.0000, 0.0000, 0.1442, 0.0000,\n",
       "         0.0000, 1.2791, 0.0000, 0.8092, 0.0000, 0.0000, 0.0000, 1.7739, 0.0000,\n",
       "         0.0000, 0.0000, 2.5908, 0.0000, 0.8842, 0.4509, 0.0000, 1.4466, 0.0000,\n",
       "         0.4791, 0.0000, 0.8701, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9088,\n",
       "         0.3542, 0.9023, 0.3203, 0.0000, 0.0000, 0.7144, 0.0000, 0.8807, 0.0000,\n",
       "         0.6550, 0.0000, 0.0000, 0.0000, 0.7872, 0.9193, 0.0000, 0.5060, 0.0000,\n",
       "         0.0000, 0.0000, 0.5826, 0.0000, 0.1698, 0.0000, 0.4970, 0.0687, 0.2020,\n",
       "         0.3487, 0.0000, 0.0000, 0.0000, 0.0000, 1.4019, 0.2377, 0.4262, 0.0000,\n",
       "         0.0000, 1.2491, 0.0000, 0.0000, 0.0000, 0.6563, 0.3966, 0.4241, 0.9908,\n",
       "         0.0000, 0.0000, 0.7215, 0.0000, 0.7379, 1.2631, 0.9568, 0.0000, 0.6891,\n",
       "         0.4715, 0.7746, 0.0000, 0.5440, 0.0000, 0.0000, 0.0000, 0.0000, 1.4656,\n",
       "         0.0000, 0.0000, 0.3301, 0.0000, 0.0000, 0.0000, 0.2432, 0.4256, 0.0000,\n",
       "         0.0000, 0.0000, 1.6484, 0.0000, 0.0000, 0.0000, 0.5188, 0.0000, 0.0000,\n",
       "         0.8149, 0.0000, 0.0000, 1.5596, 0.7682, 0.0000, 0.4984, 1.2426, 0.0155,\n",
       "         0.0000, 0.0379, 0.0000, 0.6639, 0.0000, 0.0000, 1.0070, 0.0000, 0.1959,\n",
       "         0.0542, 0.9020, 0.0000, 0.5922, 0.4179, 0.0000, 0.5554, 0.0000, 0.1609,\n",
       "         0.4818, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torch.randn(1,7,8,8)\n",
    "test_model = Encoding_Network()\n",
    "test_model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floppy-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer,num_epochs,trainloader):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss=0.0\n",
    "        \n",
    "        for i , (anchor, pos, neg) in enumerate(trainloader):\n",
    "            anchor = anchor.to(device)\n",
    "            pos = pos.to(device)\n",
    "            neg = neg.to(device)\n",
    "            \n",
    "           \n",
    "            #forward prop\n",
    "            \n",
    "            anchor_enc = model(anchor)\n",
    "            pos_enc = model(pos)\n",
    "            neg_enc = model(neg)\n",
    "            \n",
    "\n",
    "            loss = criterion(anchor_enc, pos_enc, neg_enc)\n",
    "\n",
    "            #backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            #take step\n",
    "            optimizer.step()\n",
    "            losss= loss.item()\n",
    "            \n",
    "            running_loss+=losss\n",
    "            \n",
    "            \n",
    "           \n",
    "            if i %500==499:\n",
    "                print('[%d,%5d] loss: %.3f' % (epoch+1 , i+1, running_loss/500))\n",
    "                running_loss=0.0\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "champion-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoding_Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sophisticated-companion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  500] loss: 0.007\n",
      "[1, 1000] loss: 0.019\n",
      "[1, 1500] loss: 0.010\n",
      "[1, 2000] loss: 0.004\n",
      "[1, 2500] loss: 0.020\n",
      "[1, 3000] loss: 0.006\n",
      "[1, 3500] loss: 0.010\n",
      "[1, 4000] loss: 0.020\n",
      "[1, 4500] loss: 0.008\n",
      "[1, 5000] loss: 0.009\n",
      "[1, 5500] loss: 0.007\n",
      "[1, 6000] loss: 0.004\n",
      "[1, 6500] loss: 0.020\n",
      "[1, 7000] loss: 0.006\n",
      "[1, 7500] loss: 0.009\n",
      "[1, 8000] loss: 0.008\n",
      "[1, 8500] loss: 0.017\n",
      "[1, 9000] loss: 0.006\n",
      "[1, 9500] loss: 0.007\n",
      "[1,10000] loss: 0.004\n",
      "[1,10500] loss: 0.011\n",
      "[1,11000] loss: 0.005\n",
      "[1,11500] loss: 0.013\n",
      "[1,12000] loss: 0.009\n",
      "[1,12500] loss: 0.017\n",
      "[1,13000] loss: 0.013\n",
      "[1,13500] loss: 0.018\n",
      "[1,14000] loss: 0.017\n",
      "[1,14500] loss: 0.002\n",
      "[1,15000] loss: 0.007\n",
      "[1,15500] loss: 0.011\n",
      "[1,16000] loss: 0.002\n",
      "[1,16500] loss: 0.004\n",
      "[1,17000] loss: 0.007\n",
      "[1,17500] loss: 0.006\n",
      "[1,18000] loss: 0.006\n",
      "[1,18500] loss: 0.003\n",
      "[1,19000] loss: 0.007\n",
      "[1,19500] loss: 0.008\n",
      "[1,20000] loss: 0.007\n",
      "[1,20500] loss: 0.010\n",
      "[1,21000] loss: 0.006\n",
      "[1,21500] loss: 0.010\n",
      "[1,22000] loss: 0.004\n",
      "[1,22500] loss: 0.008\n",
      "[1,23000] loss: 0.009\n",
      "[1,23500] loss: 0.006\n",
      "[1,24000] loss: 0.011\n",
      "[1,24500] loss: 0.006\n",
      "[1,25000] loss: 0.002\n",
      "[1,25500] loss: 0.009\n",
      "[1,26000] loss: 0.016\n",
      "[1,26500] loss: 0.002\n",
      "[1,27000] loss: 0.002\n",
      "[1,27500] loss: 0.005\n",
      "[1,28000] loss: 0.003\n",
      "[1,28500] loss: 0.021\n",
      "[1,29000] loss: 0.013\n",
      "[1,29500] loss: 0.012\n",
      "[1,30000] loss: 0.005\n",
      "[1,30500] loss: 0.006\n",
      "[1,31000] loss: 0.004\n",
      "[1,31500] loss: 0.008\n",
      "[1,32000] loss: 0.004\n",
      "[1,32500] loss: 0.002\n",
      "[1,33000] loss: 0.010\n",
      "[1,33500] loss: 0.004\n",
      "[1,34000] loss: 0.012\n",
      "[1,34500] loss: 0.005\n",
      "[1,35000] loss: 0.005\n",
      "[1,35500] loss: 0.003\n",
      "[1,36000] loss: 0.002\n",
      "[1,36500] loss: 0.012\n",
      "[1,37000] loss: 0.019\n",
      "[1,37500] loss: 0.008\n",
      "[1,38000] loss: 0.006\n",
      "[1,38500] loss: 0.011\n",
      "[1,39000] loss: 0.008\n",
      "[1,39500] loss: 0.007\n",
      "[1,40000] loss: 0.014\n",
      "[1,40500] loss: 0.004\n",
      "[1,41000] loss: 0.003\n",
      "[1,41500] loss: 0.004\n",
      "[1,42000] loss: 0.008\n",
      "[1,42500] loss: 0.002\n",
      "[1,43000] loss: 0.005\n",
      "[1,43500] loss: 0.004\n",
      "[1,44000] loss: 0.008\n",
      "[1,44500] loss: 0.002\n",
      "[1,45000] loss: 0.004\n",
      "[1,45500] loss: 0.003\n",
      "[1,46000] loss: 0.006\n",
      "[1,46500] loss: 0.006\n",
      "[1,47000] loss: 0.005\n",
      "[1,47500] loss: 0.006\n",
      "[1,48000] loss: 0.012\n",
      "[1,48500] loss: 0.001\n",
      "[1,49000] loss: 0.003\n",
      "[1,49500] loss: 0.005\n",
      "[1,50000] loss: 0.005\n",
      "[1,50500] loss: 0.006\n",
      "[1,51000] loss: 0.008\n",
      "[1,51500] loss: 0.003\n",
      "[1,52000] loss: 0.004\n",
      "[1,52500] loss: 0.005\n",
      "[1,53000] loss: 0.006\n",
      "[1,53500] loss: 0.010\n",
      "[1,54000] loss: 0.005\n",
      "[1,54500] loss: 0.008\n",
      "[1,55000] loss: 0.009\n",
      "[1,55500] loss: 0.003\n",
      "[1,56000] loss: 0.001\n",
      "[1,56500] loss: 0.002\n",
      "[1,57000] loss: 0.003\n",
      "[1,57500] loss: 0.009\n",
      "[1,58000] loss: 0.006\n",
      "[1,58500] loss: 0.005\n",
      "[1,59000] loss: 0.003\n",
      "[1,59500] loss: 0.008\n",
      "[1,60000] loss: 0.005\n",
      "[1,60500] loss: 0.013\n",
      "[1,61000] loss: 0.005\n",
      "[1,61500] loss: 0.016\n",
      "[1,62000] loss: 0.008\n",
      "[1,62500] loss: 0.005\n",
      "[1,63000] loss: 0.008\n",
      "[1,63500] loss: 0.004\n",
      "[1,64000] loss: 0.006\n",
      "[1,64500] loss: 0.005\n",
      "[1,65000] loss: 0.006\n",
      "[1,65500] loss: 0.006\n",
      "[1,66000] loss: 0.003\n",
      "[1,66500] loss: 0.011\n",
      "[1,67000] loss: 0.004\n",
      "[1,67500] loss: 0.004\n",
      "[1,68000] loss: 0.006\n",
      "[1,68500] loss: 0.004\n",
      "[1,69000] loss: 0.004\n",
      "[1,69500] loss: 0.006\n",
      "[1,70000] loss: 0.017\n",
      "[1,70500] loss: 0.003\n",
      "[1,71000] loss: 0.004\n",
      "[1,71500] loss: 0.014\n",
      "[1,72000] loss: 0.002\n",
      "[1,72500] loss: 0.010\n",
      "[1,73000] loss: 0.015\n",
      "[1,73500] loss: 0.007\n",
      "[1,74000] loss: 0.003\n",
      "[1,74500] loss: 0.011\n",
      "[1,75000] loss: 0.006\n",
      "[1,75500] loss: 0.016\n",
      "[1,76000] loss: 0.017\n",
      "[1,76500] loss: 0.014\n",
      "[1,77000] loss: 0.007\n",
      "[1,77500] loss: 0.007\n",
      "[1,78000] loss: 0.007\n",
      "[1,78500] loss: 0.005\n",
      "[1,79000] loss: 0.002\n",
      "[1,79500] loss: 0.009\n",
      "[1,80000] loss: 0.002\n",
      "[1,80500] loss: 0.005\n",
      "[1,81000] loss: 0.001\n",
      "[1,81500] loss: 0.006\n",
      "[1,82000] loss: 0.001\n",
      "[1,82500] loss: 0.003\n",
      "[1,83000] loss: 0.005\n",
      "[1,83500] loss: 0.001\n",
      "[1,84000] loss: 0.002\n",
      "[1,84500] loss: 0.007\n",
      "[1,85000] loss: 0.003\n",
      "[1,85500] loss: 0.008\n",
      "[1,86000] loss: 0.001\n",
      "[1,86500] loss: 0.008\n",
      "[1,87000] loss: 0.009\n",
      "[1,87500] loss: 0.010\n",
      "[1,88000] loss: 0.002\n",
      "[1,88500] loss: 0.006\n",
      "[1,89000] loss: 0.004\n",
      "[1,89500] loss: 0.003\n",
      "[1,90000] loss: 0.009\n",
      "[1,90500] loss: 0.006\n",
      "[1,91000] loss: 0.012\n",
      "[1,91500] loss: 0.006\n",
      "[1,92000] loss: 0.004\n",
      "[1,92500] loss: 0.003\n",
      "[1,93000] loss: 0.004\n",
      "[1,93500] loss: 0.002\n",
      "[1,94000] loss: 0.007\n",
      "[1,94500] loss: 0.007\n",
      "[1,95000] loss: 0.009\n",
      "[1,95500] loss: 0.008\n",
      "[1,96000] loss: 0.007\n",
      "[1,96500] loss: 0.006\n",
      "[1,97000] loss: 0.005\n",
      "[1,97500] loss: 0.003\n",
      "[1,98000] loss: 0.003\n",
      "[1,98500] loss: 0.006\n",
      "[1,99000] loss: 0.002\n",
      "[1,99500] loss: 0.017\n",
      "[1,100000] loss: 0.009\n",
      "[1,100500] loss: 0.003\n",
      "[1,101000] loss: 0.003\n",
      "[1,101500] loss: 0.008\n",
      "[1,102000] loss: 0.002\n",
      "[1,102500] loss: 0.008\n",
      "[1,103000] loss: 0.011\n",
      "[1,103500] loss: 0.002\n",
      "[1,104000] loss: 0.001\n",
      "[1,104500] loss: 0.005\n",
      "[1,105000] loss: 0.004\n",
      "[1,105500] loss: 0.002\n",
      "[1,106000] loss: 0.011\n",
      "[1,106500] loss: 0.004\n",
      "[1,107000] loss: 0.007\n",
      "[1,107500] loss: 0.004\n",
      "[1,108000] loss: 0.009\n",
      "[1,108500] loss: 0.009\n",
      "[1,109000] loss: 0.008\n",
      "[1,109500] loss: 0.005\n",
      "[1,110000] loss: 0.002\n",
      "[1,110500] loss: 0.002\n",
      "[1,111000] loss: 0.005\n",
      "[1,111500] loss: 0.016\n",
      "[1,112000] loss: 0.004\n",
      "[1,112500] loss: 0.004\n",
      "[1,113000] loss: 0.007\n",
      "[1,113500] loss: 0.008\n",
      "[1,114000] loss: 0.008\n",
      "[1,114500] loss: 0.008\n",
      "[1,115000] loss: 0.001\n",
      "[1,115500] loss: 0.004\n",
      "[1,116000] loss: 0.006\n",
      "[1,116500] loss: 0.012\n",
      "[1,117000] loss: 0.005\n",
      "[1,117500] loss: 0.008\n",
      "[1,118000] loss: 0.007\n",
      "[1,118500] loss: 0.008\n",
      "[1,119000] loss: 0.004\n",
      "[1,119500] loss: 0.009\n",
      "[1,120000] loss: 0.005\n",
      "[1,120500] loss: 0.002\n",
      "[1,121000] loss: 0.008\n",
      "[1,121500] loss: 0.005\n",
      "[1,122000] loss: 0.002\n",
      "[1,122500] loss: 0.006\n",
      "[1,123000] loss: 0.025\n",
      "[1,123500] loss: 0.002\n",
      "[1,124000] loss: 0.003\n",
      "[1,124500] loss: 0.004\n",
      "[1,125000] loss: 0.001\n",
      "[1,125500] loss: 0.011\n",
      "[1,126000] loss: 0.006\n",
      "[1,126500] loss: 0.005\n",
      "[1,127000] loss: 0.006\n",
      "[1,127500] loss: 0.006\n",
      "[1,128000] loss: 0.003\n",
      "[1,128500] loss: 0.002\n",
      "[1,129000] loss: 0.003\n",
      "[1,129500] loss: 0.004\n",
      "[1,130000] loss: 0.006\n",
      "[1,130500] loss: 0.002\n",
      "[1,131000] loss: 0.007\n",
      "[1,131500] loss: 0.007\n",
      "[1,132000] loss: 0.003\n",
      "[1,132500] loss: 0.005\n",
      "[1,133000] loss: 0.006\n",
      "[1,133500] loss: 0.010\n",
      "[1,134000] loss: 0.002\n",
      "[1,134500] loss: 0.007\n",
      "[1,135000] loss: 0.014\n",
      "[1,135500] loss: 0.002\n",
      "[1,136000] loss: 0.013\n",
      "[1,136500] loss: 0.012\n",
      "[1,137000] loss: 0.005\n",
      "[1,137500] loss: 0.002\n",
      "[1,138000] loss: 0.033\n",
      "[1,138500] loss: 0.002\n",
      "[1,139000] loss: 0.004\n",
      "[1,139500] loss: 0.008\n",
      "[1,140000] loss: 0.006\n",
      "[1,140500] loss: 0.001\n",
      "[1,141000] loss: 0.008\n",
      "[1,141500] loss: 0.008\n",
      "[1,142000] loss: 0.008\n",
      "[1,142500] loss: 0.007\n",
      "[1,143000] loss: 0.005\n",
      "[1,143500] loss: 0.002\n",
      "[1,144000] loss: 0.007\n",
      "[1,144500] loss: 0.002\n",
      "[1,145000] loss: 0.001\n",
      "[1,145500] loss: 0.001\n",
      "[1,146000] loss: 0.010\n",
      "[1,146500] loss: 0.004\n",
      "[1,147000] loss: 0.003\n",
      "[1,147500] loss: 0.003\n",
      "[1,148000] loss: 0.009\n",
      "[1,148500] loss: 0.006\n",
      "[1,149000] loss: 0.011\n",
      "[1,149500] loss: 0.003\n",
      "[1,150000] loss: 0.008\n",
      "[1,150500] loss: 0.007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#optimizer3 = optim.SGD(my_model.parameters(),lr=learning_rate,momentum=.9)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion3\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, num_epochs, trainloader)\u001b[0m\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#take step\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m losss\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     29\u001b[0m running_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mlosss\n",
      "File \u001b[1;32m~\\Documents\\Chess_Engine\\Chess_Enviorment\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Chess_Engine\\Chess_Enviorment\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Chess_Engine\\Chess_Enviorment\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\Documents\\Chess_Engine\\Chess_Enviorment\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Chess_Engine\\Chess_Enviorment\\lib\\site-packages\\torch\\optim\\adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    305\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    309\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10\n",
    "criterion3 = nn.TripletMarginLoss()\n",
    "optimizer3=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "#optimizer3 = optim.SGD(my_model.parameters(),lr=learning_rate,momentum=.9)\n",
    "num_epochs=15\n",
    "train(model,criterion3,optimizer3,num_epochs,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "peripheral-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './previous_models/Vec2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-presentation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vec1.pth -> .4\n",
    "class Encoding_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoding_Network, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(7, 32, kernel_size=2), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=2), nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2,2))\n",
    "        self.linear = nn.Linear(576, 256)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.linear(x))\n",
    "        return x\n",
    "        \n",
    "vec2 -> .005        \n",
    "class Encoding_Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Encoding_Network, self).__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(7, 32, kernel_size=2), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=2), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                                 nn.MaxPool2d(2,2))\n",
    "        self.linear = nn.Linear(576, 256)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.linear(x))\n",
    "      \n",
    "        return x\n",
    "        \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chess_Enviorment",
   "language": "python",
   "name": "chess_enviorment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
